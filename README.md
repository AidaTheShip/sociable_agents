# Sociable Agents
The goal of this project is to ultimately create sociable agents - that is agentive structures that can behave like humans. 
Modeling human-like interactions between agents in multiagent systems is emerging as a popular application for Large Language Models (LLM) and Artificial Intelligence (AI). As we are moving towards simulating human behavior in simulations with the help of generative artificial intelligence (genAI), we must consider key human and life-like factors into our modeling process. This project focuses primarily on the question whether generative multiagent systems can achieve a level of self-organization that solves problems in stagnating human-like systems, pulling wisdom from concepts in systems theory, reinforcement learning (RL), machine learning (ML), linguistics, psychology, and network science. Our hypothesis is that smart multiagent systems can simulate human-like scenarios and can find human-like or more optimal solutions than humans have found for the problems or dilemmas they will encounter.

## Systems Theory
Wherever we go, we are surrounded by systems. Systems Theory aims to describe how structures emerge, in which many people are at play to form structures larger than the individual parts. In this, interdisciplinary concepts are analyzed to understand synergy and emergent behavior.

Since we are exploring how certain system scenarios can be modeled through AI, we are going to go a bit more into the exploration of common systems theory concepts. 

### Methodologies
Systems Thinking aims at getting an overview of an issue through looking at it as an emergent phenomena that is based on the interconnections between its parts and the purpose of the system. It also models the dynamics of a system, i.e. how a system changes over time, to accommodate for it. The idea of feedback loops in systems is that they are generally time-invariant.

### Archetypes 
Here are the archetypes that we will attempt to fix through our simulation. 
1. Tragedy of Commons.
2. Fixes that fail.

There are two questions that we will explore here: Given our architecture, can we: a) end up in such a structure and b) if we do, escape it through reinforcement learning? What changed?

## The Science behind Networks
