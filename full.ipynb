{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import torch.distributions as distributions\n",
    "from tqdm import trange\n",
    "\n",
    "# Define the Policy Network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, learning_rate=0.01, gamma=0.90):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.model(state)\n",
    "\n",
    "    def predict(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Add batch dimension\n",
    "        return self.forward(state)\n",
    "\n",
    "    def update(self, states, actions, rewards, next_states):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass for current and next states\n",
    "        current_q_values = self.forward(states).gather(1, actions.unsqueeze(1))\n",
    "        next_q_values = self.forward(next_states).max(1)[0].detach()\n",
    "\n",
    "        # Compute expected Q values using the Bellman equation\n",
    "        target_q_values = rewards + self.gamma * next_q_values  # discount factor gamma = 0.99\n",
    "\n",
    "        # Calculate loss using mean squared error\n",
    "        loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # # return loss\n",
    "        # self.optimizer.zero_grad()\n",
    "        # probs = self.forward(states)\n",
    "        # log_probs = torch.log(self.forward(states))\n",
    "        # loss = -(log_probs*rewards).mean()\n",
    "        # loss.backward()\n",
    "        # self.optimizer.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Define the Agent\n",
    "class Agent:\n",
    "    def __init__(self, name, environment, id, epsilon=0.5):\n",
    "        self.name = name\n",
    "        self.id = id\n",
    "        self.social_network = environment\n",
    "        self.state_dim = 3\n",
    "        # self.action_dim = 2\n",
    "        self.action_dim = 6 # considering that we have 5 Agents and we want one of the actions to just not interact at all.\n",
    "        self.create_characteristics()\n",
    "        self.policy_net = PolicyNetwork(self.state_dim, self.action_dim)\n",
    "        self.epsilon = epsilon  # Exploration parameter\n",
    "        self.reward_history = []  # To store the reward history for this agent\n",
    "\n",
    "    def get_state(self):\n",
    "        self.update_wellbeing()\n",
    "        avg_connection_strength = self.total_weight / max(1, self.degree)\n",
    "        state = [self.introversion, self.well_being, avg_connection_strength]\n",
    "        return state\n",
    "\n",
    "    # def select_partner(self):\n",
    "    #     # Strategically select partner based on connection strength\n",
    "    #     edges = list(self.social_network.network.edges(self.name, data=True))\n",
    "    #     if not edges:\n",
    "    #         return None  # In case there are no edges\n",
    "    #     # Choose the partner with the strongest connection\n",
    "\n",
    "    #     selected_edge = max(edges, key=lambda x: x[2]['weight'])\n",
    "    #     return (selected_edge[0], selected_edge[1])\n",
    "\n",
    "\n",
    "    # def select_partner(self):\n",
    "    #   edges = list(self.social_network.network.edges(self.name, data=True))\n",
    "    #   if not edges:\n",
    "    #       return None  # In case there are no edges\n",
    "\n",
    "    #   # Random selection weighted by connection strength\n",
    "    #   total_weight = sum(data['weight'] for _, _, data in edges)\n",
    "    #   if total_weight > 0:\n",
    "    #       weights = [data['weight'] / total_weight for _, _, data in edges]\n",
    "    #       selected_edge = random.choices(edges, weights=weights, k=1)[0]\n",
    "    #   else:\n",
    "    #       # If no weights are non-zero, select randomly\n",
    "    #       selected_edge = random.choice(edges)\n",
    "\n",
    "    #   return (selected_edge[0], selected_edge[1]) # Return the partner node\n",
    "\n",
    "    # def select_partner(self):\n",
    "    #   edges = list(self.social_network.network.edges(self.name))\n",
    "    #   if not edges:\n",
    "    #       return None\n",
    "    #   return random.choice(edges)[0], random.choice(edges)[1]  # Randomly select an interaction partner\n",
    "\n",
    "\n",
    "    def action(self):\n",
    "      state = self.get_state()\n",
    "      action_prob = self.policy_net.predict(state) # These are the probabilities for interaction with Agent i, no interaction \n",
    "      print(type(action_prob), action_prob[self.id], self.id) # this is just to check whether we have the actual right action dimensionprint(action_prob)\n",
    "      # Choosing between exploration and exploitation\n",
    "      if np.random.rand() < self.epsilon:\n",
    "        action = np.random.choice(self.action_dim) # exploring what happens when you interact with an agent\n",
    "      else:\n",
    "          action = 1 \n",
    "    #   # Sampling an action from the probability distribution\n",
    "    #     action_prob[self.id] = 0.0\n",
    "    #     print(action_prob)\n",
    "    #     action_dist = distributions.Categorical(action_prob) # Look into this more deeply, change it potentially\n",
    "    #     print(action_dist)\n",
    "    #     action = action_dist.sample().item() # this is taking one of the actions and performing it\n",
    "        \n",
    "      if action == 0: # this is for the case when there is no action at all. \n",
    "          pass\n",
    "      else:\n",
    "        # interaction_partner = self.select_partner()\n",
    "        interaction_partner = f\"Agent_{action-1}\"\n",
    "        if interaction_partner != self.name:\n",
    "        # LLM will play in here in determining the quality of the conversation\n",
    "          quality_conversation = random.uniform(0,1) # random quality conversation\n",
    "          self.social_network.update_connection((self.name, interaction_partner), quality_conversation)\n",
    "\n",
    "      old_state = state\n",
    "      next_state = self.get_state()\n",
    "      self.update_wellbeing()\n",
    "      reward = self.well_being # Adding a little bit of gaussian noise here. # this is a place to be augmented as well\n",
    "\n",
    "      self.reward_history.append(reward) # Storing the reward for this state\n",
    "\n",
    "      return old_state, action, next_state, reward\n",
    "\n",
    "    #   state = self.get_state()\n",
    "    #   action_prob = self.policy_net.predict(state)\n",
    "\n",
    "    #   # Exploration-exploitation strategy: Îµ-greedy\n",
    "    #   if np.random.rand() < self.epsilon:\n",
    "    #       action = np.random.choice(self.action_dim)\n",
    "    #   else:\n",
    "    #       # Sample action from the probability distribution\n",
    "    #       action_dist = distributions.Categorical(action_prob)\n",
    "    #       action = action_dist.sample().item()\n",
    "\n",
    "    #   if action == 0:\n",
    "    #       interaction_partner = self.select_partner()\n",
    "    #       if interaction_partner:\n",
    "    #           quality_conversation = random.uniform(0, 1)  # Random quality conversation\n",
    "    #           self.social_network.update_connection(interaction_partner, quality_conversation)\n",
    "\n",
    "    #   old_state = state\n",
    "    #   next_state = self.get_state()\n",
    "    #   reward = self.well_being\n",
    "    #   self.reward_history.append(reward)  # Store the reward for this step\n",
    "    #   return old_state, action, next_state, reward\n",
    "\n",
    "    # def update_wellbeing(self):\n",
    "    #     self.degree = 0\n",
    "    #     self.total_weight = 0\n",
    "    #     for u, v in self.social_network.network.edges(self.name):\n",
    "    #         self.total_weight += self.social_network.network[u][v]['weight']\n",
    "    #         if self.social_network.network[u][v]['weight'] != 0.0:\n",
    "    #           self.degree += 1\n",
    "    #     self.sociality = self.total_weight / max(1, self.degree)\n",
    "    #     # self.sociality = 1/(1 + np.exp(-self.total_weight)) \n",
    "\n",
    "    #     # self.well_being = (1 - self.introversion) * self.sociality + self.introversion * self.degree\n",
    "    #     x = np.linspace(-10, 10, 100) \n",
    "    #     self.well_being = ((self.introversion) * self.sociality + self.introversion * max(1,self.degree))* (1/(1+np.exp(-x)))\n",
    "\n",
    "    def update_wellbeing(self):\n",
    "      self.degree = self.social_network.network.degree(self.name)\n",
    "      self.total_weight = sum(self.social_network.network[self.name][v]['weight'] for v in self.social_network.network[self.name])\n",
    "      self.sociality = self.total_weight / max(1, self.degree)\n",
    "      centrality = nx.closeness_centrality(self.social_network.network, self.name)\n",
    "      self.well_being = (self.introversion * self.sociality) + ((1 - self.introversion) * centrality)\n",
    "\n",
    "    \n",
    "    def create_characteristics(self):\n",
    "        self.introversion = random.uniform(0, 1)\n",
    "\n",
    "# Define the Social Network\n",
    "class SocialNetwork:\n",
    "    def __init__(self, agents):\n",
    "        self.participants = agents\n",
    "        self.network = nx.Graph()\n",
    "        self.history = []  # To store historical states of the network\n",
    "        for src in self.participants:\n",
    "            for dst in self.participants:\n",
    "                if src != dst:\n",
    "                    self.network.add_edge(src, dst, weight=0)\n",
    "\n",
    "    def update_connection(self, connection, strength):\n",
    "        src, dst = connection\n",
    "        self.network[src][dst]['weight'] = strength\n",
    "        self.log_change(src, dst, strength)\n",
    "\n",
    "    def log_change(self, src, dst, weight):\n",
    "        # Store the change in a log\n",
    "        self.history.append((src, dst, weight))\n",
    "\n",
    "    def snapshot(self):\n",
    "        # Take a snapshot of the entire network\n",
    "        return copy.deepcopy(self.network)\n",
    "\n",
    "    def visualize(self):\n",
    "        nt = Network(notebook=True)\n",
    "        nt.from_nx(self.network)\n",
    "        nt.show('network.html')\n",
    "\n",
    "def visualize_network(network, title=\"Social Network\"):\n",
    "    pos = nx.spring_layout(network)\n",
    "    weights = nx.get_edge_attributes(network, 'weight')\n",
    "    nx.draw(network, pos, with_labels=True, node_size=700, node_color='skyblue',\n",
    "            edge_color=[float(v) for v in weights.values()], width=4, edge_cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Setup function for agents and network\n",
    "def setup(num_agents):\n",
    "    agent_names = [f\"Agent_{i}\" for i in range(num_agents)]\n",
    "    network = SocialNetwork(agent_names)\n",
    "    agents = {agent_names[i]: Agent(agent_names[i], network, i) for i in range(num_agents)}\n",
    "    return agents, network\n",
    "\n",
    "snapshots = []\n",
    "import copy\n",
    "\n",
    "# Training function\n",
    "def train(agents, network, num_episodes, max_steps_per_episode):\n",
    "    rewards_history = {agent_name: [] for agent_name in agents.keys()}\n",
    "    network_evolution = []\n",
    "    decay = 0.955\n",
    "\n",
    "    for episode in trange(num_episodes):\n",
    "        episode_network = {}\n",
    "        if episode % 10 == 0:\n",
    "            snapshots.append(network.snapshot())\n",
    "\n",
    "        for agent_name, agent in agents.items():\n",
    "            total_reward = 0\n",
    "            for step in range(max_steps_per_episode):\n",
    "                old_state, action, next_state, reward = agent.action()\n",
    "                agent.policy_net.update(torch.tensor([old_state], dtype=torch.float32),\n",
    "                                         torch.tensor([action], dtype=torch.int64),\n",
    "                                         torch.tensor([reward], dtype=torch.float32),\n",
    "                                         torch.tensor([next_state], dtype=torch.float32))\n",
    "                total_reward += reward\n",
    "            rewards_history[agent_name].append(total_reward)\n",
    "            episode_network[agent_name] = np.array(agent.get_state())\n",
    "            agents[agent_name].epsilon *= decay\n",
    "\n",
    "        network_evolution.append(episode_network)\n",
    "\n",
    "    df_network_evolution = pd.DataFrame(network_evolution)\n",
    "\n",
    "    for agent_name, rewards in rewards_history.items():\n",
    "        plt.plot(np.log(rewards))\n",
    "        plt.title(f'Total Rewards per Episode for agent {agent_name}')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.show()\n",
    "\n",
    "    visualize_network(snapshots[0], \"Network at Start\")\n",
    "    visualize_network(snapshots[-1], \"Network at End\")\n",
    "    return rewards_history\n",
    "\n",
    "# Assuming we have 5 agents\n",
    "agents, network = setup(10)\n",
    "history_rewards = train(agents, network, 300, 100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
